{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e45438a",
   "metadata": {},
   "source": [
    "# Read Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5dfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read the training csv file and assign training set\n",
    "df = pd.read_csv('en_train.csv')\n",
    "trainAudio = df['LINK']\n",
    "y_train = df['WORD']\n",
    "\n",
    "#read the testing csv file and assign testing set\n",
    "df2 = pd.read_csv('en_test.csv')\n",
    "testAudio = df2['LINK']\n",
    "y_test = df2['WORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e8aa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINK</th>\n",
       "      <th>WORD</th>\n",
       "      <th>VALID</th>\n",
       "      <th>SPEAKER</th>\n",
       "      <th>GENDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachen/common_voice_en_19779773.wav</td>\n",
       "      <td>aachen</td>\n",
       "      <td>True</td>\n",
       "      <td>0aedd0e24f1db5b1ce965f107fd19dd40e0c50f7ead449...</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aachen/common_voice_en_19798768.wav</td>\n",
       "      <td>aachen</td>\n",
       "      <td>True</td>\n",
       "      <td>ed0e4d79c6c2889459e88e11724dbd7f2cb2417e6a4320...</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aachen/common_voice_en_19852665.wav</td>\n",
       "      <td>aachen</td>\n",
       "      <td>True</td>\n",
       "      <td>cd185c1ab8659ae6d21d7b63dc4a5e54a3f65f98e29ff1...</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aachen/common_voice_en_20127845.wav</td>\n",
       "      <td>aachen</td>\n",
       "      <td>True</td>\n",
       "      <td>29b8505586cd43382cd695da6b943f401104be710a5b60...</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aachen/common_voice_en_20449666.wav</td>\n",
       "      <td>aachen</td>\n",
       "      <td>True</td>\n",
       "      <td>372293e65cdab88771e028a4351651ab2eff64438ddafc...</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  LINK    WORD  VALID  \\\n",
       "0  aachen/common_voice_en_19779773.wav  aachen   True   \n",
       "1  aachen/common_voice_en_19798768.wav  aachen   True   \n",
       "2  aachen/common_voice_en_19852665.wav  aachen   True   \n",
       "3  aachen/common_voice_en_20127845.wav  aachen   True   \n",
       "4  aachen/common_voice_en_20449666.wav  aachen   True   \n",
       "\n",
       "                                             SPEAKER  GENDER  \n",
       "0  0aedd0e24f1db5b1ce965f107fd19dd40e0c50f7ead449...    MALE  \n",
       "1  ed0e4d79c6c2889459e88e11724dbd7f2cb2417e6a4320...  FEMALE  \n",
       "2  cd185c1ab8659ae6d21d7b63dc4a5e54a3f65f98e29ff1...    MALE  \n",
       "3  29b8505586cd43382cd695da6b943f401104be710a5b60...  FEMALE  \n",
       "4  372293e65cdab88771e028a4351651ab2eff64438ddafc...    MALE  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e45b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38173"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "\n",
    "#number of classes\n",
    "path = 'en/clips/'\n",
    "folders = len(os.listdir(path))\n",
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bcd43",
   "metadata": {},
   "source": [
    "# Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc05387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# paths = Path('en/clips/')\n",
    "\n",
    "# for f in paths.iterdir():\n",
    "#     for f2 in f.iterdir():\n",
    "#         if f2.is_file() and f2.suffix in ['.opus']:\n",
    "#             f2.rename(f2.with_suffix('.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29298dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioToSpectrogram(filename):\n",
    "    y,sr=librosa.load(filename, sr=16000) #load the file\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr,n_mels=128,f_min=20,f_max=sr)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e67adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the training audios to spectograms and place it in the Xtrain variable\n",
    "for x in range(len(df)):\n",
    "    filename= path + str(trainAudio[x]) #get the filename \n",
    "    spectrogram = AudioToSpectrogram(filename)\n",
    "    X_train.append(spectrogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d57f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert the testing audios to spectograms and place it in the Xtest variable\n",
    "for x in range(len(df2)):\n",
    "    filename= path + str(testAudio[x]) #get the filename \n",
    "    spectrogram = AudioToSpectrogram(filename)\n",
    "    X_test.append(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator as IDG\n",
    "trainDatagen = IDG(rescale = 1/255.0)\n",
    "testDatagen = IDG(rescale = 1/255.0)\n",
    "\n",
    "directory = \"en/clips\"\n",
    "height = 256\n",
    "width = 256\n",
    "batchSize = 32\n",
    "\n",
    "trainGen = trainDatagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(height, width),\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "testGen = testDatagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(height, width),\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacks = EarlyStopping(monitor = 'val_loss', patience = '10', verbose = '1', mode = 'auto')\n",
    "modelPath = 'model.h5'\n",
    "h5Model = ModelCheckpoint(modelPath, monitor = 'val_accuracy', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee6359",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938895a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "model = Sequential([\n",
    "    #first set of convolution and pooling layer\n",
    "    Conv2D(32,(3,3), activation = 'relu', input_shape = (250, 250, 1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    #second set of convolution and pooling layer\n",
    "    Conv2D(32,(3,3), activation = 'relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    #third set of convolution and pooling layer\n",
    "    Conv2D(32,(3,3), activation = 'relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    #FC layer\n",
    "    Flatten(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    #final layer, input is number of classes\n",
    "    Dense(folders, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#model compilation\n",
    "model.compile(optimizer='adam', loss='categorical_cross_entropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c353c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17153c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "history = model.fit(trainGen,\n",
    "                    validation_data = testDatagen,\n",
    "                    batch_size = batchSize,\n",
    "                    epochs = 10,\n",
    "                    verbose = 1,\n",
    "                    callbacks =[h5Model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04ae2f",
   "metadata": {},
   "source": [
    "# Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Initialising basic values\n",
    "accuracy = history.model_training['accuracy']\n",
    "val_accuracy = history.model_training['val_accuracy']\n",
    "loss = history.model_training['loss']\n",
    "val_loss = history.model_training['val_loss']\n",
    "epochs=range(len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57573ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the graph for the accuracy of training and testing\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "plt.plot(epochs, accuracy, 'r', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_accuracy, 'b', label=\"Test Accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "#Plotting the graph for the loss of training and testing\n",
    "fig2 = plt.figure(figsize=(14,7))\n",
    "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Test Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab6e86",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'predict/prediction1.vlc'\n",
    "SpectrogramImageForPrediction = AudioToSpectrogram(filename)\n",
    "model.predict(SpectrogramImageForPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "sound_file = AudioSegment.from_wav(\"predict/prediction2.wav\")\n",
    "audio_chunks = split_on_silence(sound_file, \n",
    "    # must be silent for at least half a second\n",
    "    min_silence_len=500,\n",
    "\n",
    "    # consider it silent if quieter than -16 dBFS\n",
    "    silence_thresh=-16\n",
    ")\n",
    "\n",
    "SpectrogramImagesForPrediction = []\n",
    "for x in audio_chunks:\n",
    "    SpectrogramImageForPrediction = AudioToSpectrogram(filename)\n",
    "    SpectrogramImagesForPrediction.append(SpectrogramImageForPrediction)\n",
    "    \n",
    "for x in SpectrogramImagesForPrediction:\n",
    "    model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96389fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9479ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7def8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046da798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2c77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1757e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29fe20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493bf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779ab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e82d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b8018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d5c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a874bd7150d1b4f1a50db2331c25ba57727a271980424b90318db50176dd20d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
